"""config.py"""
import numpy as np
import torch

# Removed TEST_BSZ - batch size configs removed (training-only)
# yapf: disable
"""
audio_cfg:
- Used by 'ymt3' to create a spectrogram layer.
- Input shape of model is determined by audio_cfg.
- 'train.py' arguments can override these defaults.
"""
audio_cfg = {
    # Overwrittable by args in train.py
    "codec": "melspec",  # {melspec, spec} melspec for MT3, spec for PerceiverTF
    "hop_length": 128,  # {128, 300} 128 for MT3, 300 for PerceiverTF
    # Shared audio parameters
    "audio_backend": "torchaudio",  # {torchaudio, nnAudio}
    "sample_rate": 16000,
    "input_frames": 32767, # number of input frames (~=2.048 s), determining in-/output shape of front layers. 
    "n_fft": 2048,
    "n_mels": 512,  # only for melspec
    "f_min": 50.0,
    "f_max": 8000.0,
} # TODO: currently dataloader is not updated by "input_frames"

"""
model_cfg:
- Encoder type dictates use of T5_CFG or PERCEIVER_TF_CFG.
- 'train.py' arguments can override these defaults.
"""
model_cfg = {
    "encoder_type": "t5",  # {"t5", "perceiver-tf", "conformer"}
    "decoder_type": "t5", # {"t5", "multi-t5"}
    "pre_encoder_type": "default",  # {None, "default", "conv", "conv1d", "conv2d_avpt"} by default, t5:None, perceiver:conv.
    "pre_encoder_type_default": {"t5": None, "perceiver-tf": "conv", "conformer": None},
    "pre_decoder_type": "default", # {None, 'linear', 'conv1', 'mlp', 'group_linear'} see model/projection_layer.py
    "pre_decoder_type_default": { # [enc_type][dec_type]
        "t5": {"t5": None,},
        "perceiver-tf": {"t5": "linear", "multi-t5": "mc_shared_linear"},
        "conformer": {"t5": None,},
    },
    "conv_out_channels": 128, # number of filters for 'conv' pre_encoder. Otherwise ignored.
    "t5_basename": "google/t5-v1_1-small",
    "pretrained": False, # bool, if True, load pretrained weights from t5_basename. Mismatched layers are ignored.
    "use_task_conditional_encoder": True, # True by default, but default task is None. So not activated by default. 
    "use_task_conditional_decoder": True, # True by default, but default task is None. So not activated by default.  
    "d_feat": "auto", # Input audio feature dimension for encoder. Automatically inferred by audio_cfg and existence of pre_encoders.
    "tie_word_embeddings": True, # If True, weights of embed_tokens and lm_head are tied for stabilizing gradients. 
    "vocab_size": "auto", # int or "auto", automatically inferred by task manager.
    "num_max_positions": "auto", # int or "auto". Length of positional encoding. Automatically inferred by "feat_length", "event_length" and task_manager.max_task_token_length.
    # 'vocab_size', 'tie_word_embeddings' and 'num_max_positions' are auto-copied to encoder and decoder configs in the below.
    "encoder": {
        "t5": {
            "d_model": 512, # Hidden size of T5 encoder. 
            "num_heads": 6,
            "num_layers": 8,
            "dropout_rate": 0.05,
            "position_encoding_type": "sinusoidal", # {'sinusoidal', 'trainable'}.
            "ff_widening_factor": 2, # wideening factor for MLP/MoE layers. Default is 2 in T5.
            "ff_layer_type": "t5_gmlp", # {'t5_gmlp', 'moe', 'mlp', 'gmlp'}. 'moe' for mixture of experts, 'mlp' for standard transformer dense layer, 'gmlp' for simple gated MLP.
        },
        "perceiver-tf": {
            "num_latents": 24, # number of latents in Perceiver. 24 in perceiver-tf paper.
            "d_latent": 128, # latent dimension of Perceiver. 128 in perceiver-tf paper.
            "d_model": "q", # int or "q" or "kv". Inner-dim of sca and local/temporal self-att.
                # "q" follows "latent_dim". "kv" follows  "d_feat". Best practice is to inc-/decrease 'd_latent', instead of 'd_model'.
            "num_blocks": 3, # number of Perceiver-TF blocks in encoder. L in the paper.
            "num_local_transformers_per_block": 2, # N in the paper.
            "num_temporal_transformers_per_block": 2,  # M in the paper.
            "sca_use_query_residual": False,
            "dropout_rate": 0.1,
            "position_encoding_type": "trainable", # {'trainable', 'rotary', 'alibi', 'alibit', None, 'tkd','td', 'tk', 'kdt'}. alibit is alibi with trainable slopes.
            "attention_to_channel": True, # Whether to use channel attention in sca.
            "layer_norm_type": "layer_norm", # {'layer_norm', 'rms_norm'}
            "ff_layer_type": "mlp", # {'moe', 'mlp', gmlp}. 'moe' for mixture of experts, 'mlp' for standard transformer dense layer, 'gmlp' for simple gated MLP.
            "ff_widening_factor": 1, # wideening factor for MLP/MoE layers. Default is 1.
            "moe_num_experts": 4, # number of experts in MoE layer. Default is 4. Disabled if ff_layer_type is not 'moe'.
            "moe_topk": 2, # top-k routing in MoE layer. Default is 2. Disabled if ff_layer_type is not 'moe'.
            "hidden_act": 'gelu', # activation function in MLP/MoE layer. Default is 'gelu'. {'gelu', 'silu', 'relu'}
            "rotary_type_sca": "pixel", # {'l'|'lang', 'p'|'pixel'}. Default is 'pixel'.
            "rotary_type_latent": "pixel", # {'l'|'lang', 'p'|'pixel'}. Default is 'pixel'.
            "rotary_type_temporal": "lang", # {'l'|'lang', 'p'|'pixel'}. Default is 'lang'.
            "rotary_apply_to_keys": False, # Whether to apply rotary to keys. Default is False.
            "rotary_partial_pe": False, # Whether to use partial positional encoding. Default is False.
        },
        "conformer": {
            "d_model": 512, # Hidden size of T5 encoder. 
            "intermediate_size": 512, # or 2048. size of the intermediate feed forward layer in each T5Block
            "num_heads": 8,
            "num_layers": 8,
            "dropout_rate": 0.1,
            "layerdrop": 0.1, # see https://arxiv.org/abs/1909.11556
            "position_encoding_type": "rotary", # {'rotary', 'relative'}. 
            "conv_dim": (512, 512, 512, 512, 512, 512, 512),
            "conv_stride": (5, 2, 2, 2, 2, 2, 2),
            "conv_kernel": (10, 3, 3, 3, 3, 3, 3),
            "conv_depthwise_kernel_size": 31,
        },

    },
    "decoder": {
        "t5": {
            "d_model": 512, # Hidden size of T5 encoder. If encoder has lower dim, it is projected to this dim for enc-dec cross att.
            "num_heads": 6,
            "num_layers": 8,
            "dropout_rate": 0.05,
            "position_encoding_type": "sinusoidal", # {'sinusoidal', 'trainable'}.
            "ff_widening_factor": 2, # wideening factor for MLP/MoE layers. Default is 2 in T5.
            "ff_layer_type": "t5_gmlp", # {'t5_gmlp', 'moe', 'mlp', 'gmlp'}. 'moe' for mixture of experts, 'mlp' for standard transformer dense layer, 'gmlp' for simple gated MLP.
        },
        "multi-t5": {
            "d_model": 512, # Hidden size of T5 encoder. Recommended: {256 or 512}
            "num_heads": 6,
            "num_layers": 8,
            "dropout_rate": 0.05,
            "position_encoding_type": "sinusoidal", # {'sinusoidal', 'trainable'}.
            "ff_widening_factor": 2, # wideening factor for MLP/MoE layers. Default is 2 in T5.
            "ff_layer_type": "t5_gmlp", # {'t5_gmlp', 'moe', 'mlp', 'gmlp'}. 'moe' for mixture of experts, 'mlp' for standard transformer dense layer, 'gmlp' for simple gated MLP.
            "num_channels": 13,
        },
    },
    "feat_length": "auto", # Input audio feature length for encoder. Automatically inferred by audio_cfg.
        # mt3: 256 time steps
    "event_length": 1024,  # max length of event tokens excluding task tokens <-- 128 for multi-t5
    "init_factor": 1.0, # initialization factor for embedding layers
}

# yapf: enable
# Minimal shared_cfg for inference-only usage
# Removed all training-related configs: BSZ, AUGMENTATION, DATAIO, CHECKPOINT, TRAINER, WANDB, LR_SCHEDULE
shared_cfg = {
    # PATH kept for backward compatibility (used by eval datasets, but not core inference)
    "PATH": {
        "data_home": "../../data", # path to the data directory. If using relative path, it is relative to /src directory.
    },
    # TOKENIZER config kept (optional, tokenizers have defaults)
    "TOKENIZER": {
        "max_shift_steps": "auto", # max number of shift steps in the model. (int) or "auto". If "auto", it is set by audio_cfg["input_frames"] and shift_steps_ms. 206 with default setup.
        "shift_step_ms": 10, # shift step in ms
    },
}

T5_BASE_CFG = {
    "google/t5-v1_1-small": {
        "architectures": ["T5ForConditionalGeneration"],
        "d_ff":
            1024,  # size of the intermediate feed forward layer in each T5Block. Can be overwrten by ff_widening_factor in model_cfg.
        "d_kv": 64,  # d_kv has to be equal to d_model // num_heads.
        # "d_model": 512,  # encoder hiddnen size, defined by model_cfg
        "decoder_start_token_id": 0,
        "dense_act_fn": "gelu_new",
        # "dropout_rate": 0.05,  # can be overwritten by args in ymt3
        "eos_token_id": 1,
        "feed_forward_proj": "gated-gelu",
        "initializer_factor": 1.0,
        "is_encoder_decoder": True,
        "is_gated_act": True,
        "layer_norm_epsilon": 1e-06,
        "model_type": "t5",
        # "num_decoder_layers": 8, # defined by model_cfg
        # "num_heads": 6,  # defined by model_cfg
        # "num_layers": 8,  # defined by model_cfg
        "output_past": True,
        "pad_token_id": 0,
        "relative_attention_num_buckets": 32,
        # "tie_word_embeddings": True,
        "use_cache": True,
        # "vocab_size": 1391 # vocab_size is automatically set by the task manager...
    },
    "google/t5-efficient-small": {
        "architectures": ["T5ForConditionalGeneration"],
        "d_ff": 2048,
        "d_kv": 64,
        "d_model": 512,
        "decoder_start_token_id": 0,
        "dropout_rate": 0.1,
        "eos_token_id": 1,
        "feed_forward_proj": "relu",
        "initializer_factor": 1.0,
        "is_encoder_decoder": True,
        "layer_norm_epsilon": 1e-06,
        "model_type": "t5",
        "num_decoder_layers": 6,
        "num_heads": 8,
        "num_layers": 6,
        "pad_token_id": 0,
        "relative_attention_num_buckets": 32,
        "torch_dtype": "float32",
        "transformers_version": "4.17.0.dev0",
        "use_cache": True,
    },
}

# yapf: enable
# Removed DEEPSPEED_CFG - DeepSpeed is training-only, not needed for inference
